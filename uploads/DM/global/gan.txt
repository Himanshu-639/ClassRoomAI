Generative Adversarial Networks (GANs) are AI systems that can create entirely new content—images, videos, or stories—that are not copied from existing data but are imagined by the model. A GAN learns patterns from a training dataset and then generates new combinations that often go beyond what it has seen before. This makes GANs especially useful in creative fields like digital art, game design, and animation, where novelty and uniqueness are highly valued.


While explainable AI (XAI) aims to make AI decisions understandable, applying this idea too strictly to GANs can become a problem. Trying to explain every step of a GAN’s creative process—like why a particular texture was added to a generated image, or why a fantasy character has three wings instead of two—is not only difficult, but it also limits the model's freedom.

To make models more explainable, developers often need to reduce their complexity or add constraints that simplify how they work. But in doing so, we risk making the output more repetitive, boring, or similar to existing examples. The result is a loss of creativity and performance, especially in tasks where uniqueness is more important than interpretability.


This trade-off can be better understood by looking at human creativity. Take fairy tales, for example. Stories about talking animals, magical forests, or flying carpets often come from imagination, not logic. Most authors or children invent these ideas without being able to explain where they came from. They don’t follow a step-by-step process that can be traced or justified.

Yet we don’t criticize these stories for being unexplainable—we appreciate them because they are imaginative and unpredictable. In fact, it’s the unexplainability of human creativity that makes it authentic and emotionally powerful.


GANs work in a similar way. When they generate surprising or dream-like images, they are not doing so by following clear, human-readable rules. Instead, they’re navigating a complex and often abstract space of learned features. Just like a person might dream up a world that’s never been seen before, GANs can create original ideas that feel new and exciting.

Expecting GANs to explain every step of this process is like asking an artist to justify every brushstroke—it can interrupt the flow and miss the bigger picture.


There’s a clear trade-off between performance (in terms of creativity, novelty, and quality) and explainability (how understandable the model's process is). In high-stakes applications like healthcare or finance, explainability is essential. But in creative domains, insisting on explanations can reduce the value of the output.

This idea is supported even in critical fields like healthcare, where researchers have pointed out that explanations must match the audience’s level of understanding. For instance, patients with low medical knowledge benefit from simple explanations and visual aids, while doctors may prefer detailed, technical reasoning. This proves that explainability is not one-size-fits-all—it depends on who’s receiving the explanation. In the same way, people interacting with AI in creative fields—like artists, game designers, or audiences—aren’t looking for technical justifications. They value surprise, emotion, and originality. Forcing full explanations in these settings could actually take away from the experience rather than enhance it.

In creative AI, it’s often better to embrace unexplainability—to allow the system to explore, invent, and surprise us. GANs thrive in these situations not in spite of their opacity, but because of it. Their unexplainable nature mirrors the same mystery and magic we find in human creativity.



Enhacing Human Creativity with Aptly Uncontrollable Generative AI